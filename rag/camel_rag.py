"""
CamelRAG - åŸºäºCAMELæ¡†æ¶çš„æ‰‹åŠ¨å‘é‡åŒ–å¤„ç†å·¥å…·

æ­¤æ¨¡å—ç”¨äºæ‰‹åŠ¨å¤„ç†å’Œåˆå§‹åŒ–RAGæ•°æ®ï¼Œé€‚ç”¨äºï¼š
- æ‰¹é‡å¤„ç†ç»“æ„åŒ–JSONæ•°æ®
- è‡ªå®šä¹‰chunkingç­–ç•¥çš„æ•°æ®å‘é‡åŒ–
- åˆå§‹åŒ–ç‰¹å®šæ ¼å¼çš„çŸ¥è¯†åº“

æ³¨æ„ï¼šè¿™æ˜¯æ‰‹åŠ¨å¤„ç†å·¥å…·ï¼Œä¸ç”¨äºç”Ÿäº§ç¯å¢ƒçš„è‡ªåŠ¨åŒ–RAGæœåŠ¡
"""
import json
import time
from embeddings import chunk_data_by_title, chunk_data_for_log
from camel.embeddings import SentenceTransformerEncoder
from camel.storages import QdrantStorage
from camel.retrievers import VectorRetriever
import os
from transformers import AutoTokenizer
import logging
import torch
import gc
from typing import Optional, List, Dict, Any, Callable

logger = logging.getLogger("Camel_RAG")
logger.setLevel(logging.INFO)

class CamelRAG:
    """åŸºäºCAMELæ¡†æ¶çš„RAGæ‰‹åŠ¨å¤„ç†å·¥å…·"""
    
    def __init__(
        self, 
        collection_name: str,
        embedding_model_path: str = "models/multilingual-e5-large",
        vector_storage_path: str = "data/vector_data"
    ):
        """åˆå§‹åŒ–CamelRAG
        
        Args:
            collection_name: å‘é‡é›†åˆåç§°
            embedding_model_path: Embeddingæ¨¡å‹è·¯å¾„
            vector_storage_path: å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„
        """
        self.collection_name = collection_name
        self.embedding_model_path = embedding_model_path
        self.vector_storage_path = vector_storage_path
        
        # åˆå§‹åŒ–é…ç½®
        self.config = {
            "vector_top_k": 5,
            "similarity_threshold": 0.6,
            "chunk_size": 500,
            "chunk_overlap": 50
        }
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()

    def _initialize_components(self):
        """åˆå§‹åŒ–CAMEL RAGç»„ä»¶"""
        try:
            logger.info(f"åˆå§‹åŒ–CAMEL RAGç»„ä»¶: {self.collection_name}")
            
            # åˆå§‹åŒ–Tokenizer
            logger.info(f"åŠ è½½Tokenizer: {self.embedding_model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.embedding_model_path)
            
            # åˆå§‹åŒ–Embeddingæ¨¡å‹
            logger.info("åˆå§‹åŒ–SentenceTransformerEncoder")
            self.embedding_instance = SentenceTransformerEncoder(
                model_name=self.embedding_model_path
            )
            
            # åˆå§‹åŒ–å‘é‡å­˜å‚¨
            logger.info(f"åˆå§‹åŒ–QdrantStorage: {self.collection_name}")
            self.vector_storage = QdrantStorage(
                collection_name=self.collection_name,
                path=self.vector_storage_path,
                embedding_dim=1024  # multilingual-e5-largeçš„å‘é‡ç»´åº¦
            )
            
            # åˆå§‹åŒ–æ£€ç´¢å™¨
            logger.info("åˆå§‹åŒ–VectorRetriever")
            self.vr = VectorRetriever(
                embedding_model=self.embedding_instance,
                storage=self.vector_storage
            )
            
            logger.info("âœ… CAMEL RAGç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–CAMEL RAGç»„ä»¶å¤±è´¥: {e}")
            raise
    
    def embedding(
        self, 
        data_path: Optional[str] = None, 
        data: Optional[List[Dict]] = None, 
        chunk_type: Callable = chunk_data_by_title, 
        max_tokens: int = 500
    ):
        """å‘é‡åŒ–ç»“æ„åŒ–æ•°æ®
        
        Args:
            data_path: JSONæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä¸dataäºŒé€‰ä¸€ï¼‰
            data: ç»“æ„åŒ–æ•°æ®åˆ—è¡¨ï¼ˆä¸data_pathäºŒé€‰ä¸€ï¼‰
            chunk_type: chunkingå‡½æ•°ï¼ˆchunk_data_by_titleæˆ–chunk_data_for_logï¼‰
            max_tokens: æ¯ä¸ªchunkçš„æœ€å¤§tokenæ•°
        """
        if data is None and data_path is None:
            raise ValueError("å¿…é¡»æä¾›data_pathæˆ–dataå‚æ•°")
        
        # åŠ è½½æ•°æ®
        if data is None:
            logger.info(f"ä»æ–‡ä»¶åŠ è½½æ•°æ®: {data_path}")
            with open(data_path, "r", encoding="utf-8") as f:
                structured_data = json.load(f)
        else:
            structured_data = data
        
        logger.info(f"æ•°æ®é¡¹æ•°é‡: {len(structured_data)}")
        
        # åˆ†å—å¤„ç†
        logger.info(f"ä½¿ç”¨chunkingå‡½æ•°: {chunk_type.__name__}, max_tokens={max_tokens}")
        chunks = chunk_type(
            structured_data,
            MAX_TOKENS=max_tokens,
            tokenizer=self.tokenizer,
        )
        
        logger.info(f"ç”Ÿæˆäº† {len(chunks)} ä¸ªchunks")
        
        # å‘é‡åŒ–å¹¶å­˜å‚¨
        start_time = time.time()
        for i, chunk in enumerate(chunks):
            if (i + 1) % 10 == 0:
                logger.info(f"å¤„ç†è¿›åº¦: {i+1}/{len(chunks)}")
            
            self.vr.process(
                content=chunk["content"],
                should_chunk=False,
                extra_info={
                    "id": chunk["chunk_id"], 
                    "title": chunk.get("title", ""), 
                    "type": chunk.get("type", "text")
                }
            )
        
        end_time = time.time()
        elapsed = end_time - start_time
        logger.info(f"âœ… å‘é‡åŒ–å®Œæˆï¼å…±å¤„ç† {len(chunks)} ä¸ªchunks")
        logger.info(f"â±ï¸  è€—æ—¶: {elapsed:.2f}ç§’ (å¹³å‡ {elapsed/len(chunks):.3f}ç§’/chunk)")
        
        if data_path:
            logger.info(f"ğŸ“„ æ•°æ®æº: {data_path}")

    def embedding_auto(self, data: List[str]):
        """è‡ªåŠ¨å‘é‡åŒ–æ–‡æœ¬åˆ—è¡¨ï¼ˆæ— éœ€chunkingï¼‰
        
        Args:
            data: æ–‡æœ¬åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹è‡ªåŠ¨å‘é‡åŒ– {len(data)} ä¸ªæ–‡æœ¬")
        start_time = time.time()
        
        for i, chunk in enumerate(data):
            if (i + 1) % 10 == 0:
                logger.info(f"å¤„ç†è¿›åº¦: {i+1}/{len(data)}")
            self.vr.process(content=chunk, should_chunk=False)

        end_time = time.time()
        elapsed = end_time - start_time
        logger.info(f"âœ… è‡ªåŠ¨å‘é‡åŒ–å®Œæˆï¼")
        logger.info(f"â±ï¸  è€—æ—¶: {elapsed:.2f}ç§’")

    def rag_retrieve(self, query: str, topk: Optional[int] = None) -> List[str]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            topk: è¿”å›top-kç»“æœï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼ï¼‰
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸ” RAGæ£€ç´¢å¼€å§‹")
        logger.info(f"   æŸ¥è¯¢: {query}")
        
        top_k = topk if topk is not None else self.config.get("vector_top_k", 5)
        similarity_threshold = self.config.get("similarity_threshold", 0.6)
        
        logger.info(f"   å‚æ•°: top_k={top_k}, threshold={similarity_threshold}")
        
        results = self.vr.query(
            query=query, 
            top_k=top_k, 
            similarity_threshold=similarity_threshold
        )
        
        retrieved = []
        for i, info in enumerate(results):
            retrieved.append(f"{i+1}. {info['text']}\n\n")
        
        logger.info(f"âœ… æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(retrieved)} ä¸ªç»“æœ")
        
        # åªåœ¨DEBUGæ¨¡å¼ä¸‹æ‰“å°è¯¦ç»†ç»“æœ
        if logger.level <= logging.DEBUG:
            logger.debug("æ£€ç´¢ç»“æœè¯¦æƒ…:")
            for r in retrieved:
                logger.debug(r[:200] + "..." if len(r) > 200 else r)
        
        return retrieved
    
    def release(self):
        """é‡Šæ”¾èµ„æº"""
        logger.info("å¼€å§‹é‡Šæ”¾ RAG å ç”¨çš„èµ„æº")
        
        # æ¸…ç†å¼•ç”¨
        self.vr = None 
        self.vector_storage = None
        self.embedding_instance = None
        self.tokenizer = None
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        logger.info("âœ… èµ„æºé‡Šæ”¾å®Œæˆ")

